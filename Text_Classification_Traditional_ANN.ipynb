{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51e162d4",
   "metadata": {},
   "source": [
    "# Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d64b5166",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00384bec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.7.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85619b0",
   "metadata": {},
   "source": [
    "# Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "780b06a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Context.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e0b40ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The eternal mystique of Goldman Sachs</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Either you don't care enough to actually tell ...</td>\n",
       "      <td>Love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I am such an IDIOT.</td>\n",
       "      <td>Heavy Emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>While lifting weights on Friday and doing bent...</td>\n",
       "      <td>Health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Something's watching me</td>\n",
       "      <td>Animals</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text        Context\n",
       "0              The eternal mystique of Goldman Sachs       Politics\n",
       "1  Either you don't care enough to actually tell ...           Love\n",
       "2                                I am such an IDIOT.  Heavy Emotion\n",
       "3  While lifting weights on Friday and doing bent...         Health\n",
       "4                            Something's watching me        Animals"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "018dd0bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31386, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11fbb921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 31386 entries, 0 to 31385\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   Text     31386 non-null  object\n",
      " 1   Context  31386 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 490.5+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d40389d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Heavy Emotion    3674\n",
       "Religion         3466\n",
       "Love             3229\n",
       "Self             3105\n",
       "Compliment       3061\n",
       "Animals          2622\n",
       "Health           2595\n",
       "Education        2534\n",
       "Joke             2476\n",
       "Science          2428\n",
       "Politics         2196\n",
       "Name: Context, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.Context.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3809c7a5",
   "metadata": {},
   "source": [
    "# Cleaning the Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c85fcc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_character_remover = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "extra_symbol_remover = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0583145f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = special_character_remover.sub(' ', text)\n",
    "    text = extra_symbol_remover.sub('', text)\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS)\n",
    "    return text\n",
    "    \n",
    "dataset['Text'] = dataset['Text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cc642cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "405759\n"
     ]
    }
   ],
   "source": [
    "print(dataset['Text'].apply(lambda x: len(x.split(' '))).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c08fb99",
   "metadata": {},
   "source": [
    "# Finding Maximum Sequence Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc3a50a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "g=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60b1736b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dataset['Text']:\n",
    "    g.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c842781e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31386"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c56b584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length in the list of sentences: 4624\n"
     ]
    }
   ],
   "source": [
    "maxl = max([len(s) for s in g])\n",
    "print ('Maximum sequence length in the list of sentences:', maxl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2af1001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 39335 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "MAX_NB_WORDS = 50000\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 500\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(dataset['Text'].values)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab54edc",
   "metadata": {},
   "source": [
    "# Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b6ca5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (31386, 500)\n"
     ]
    }
   ],
   "source": [
    "X = tokenizer.texts_to_sequences(dataset['Text'].values)\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd5452db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of label tensor: (31386, 11)\n"
     ]
    }
   ],
   "source": [
    "Y = pd.get_dummies(dataset['Context']).values\n",
    "print('Shape of label tensor:', Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d41177",
   "metadata": {},
   "source": [
    "# Train and Test Split up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50888cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28247, 500) (28247, 11)\n",
      "(3139, 500) (3139, 11)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.10, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6b1a26",
   "metadata": {},
   "source": [
    "# Building the ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d00a1d",
   "metadata": {},
   "source": [
    "Initializing the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "277111bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd232de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.add(tf.keras.layers.Dense(units=66, activation ='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7a6af30",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.add(tf.keras.layers.Dense(units=66, activation ='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "acf8be55",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.add(tf.keras.layers.Dense(units=11, activation ='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457b478b",
   "metadata": {},
   "source": [
    "# Training the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "433e2594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "111/111 [==============================] - 0s 2ms/step - loss: 226.2055 - accuracy: 0.1050\n",
      "Epoch 2/20\n",
      "111/111 [==============================] - 0s 2ms/step - loss: 41.2137 - accuracy: 0.1130\n",
      "Epoch 3/20\n",
      "111/111 [==============================] - 0s 2ms/step - loss: 5.4506 - accuracy: 0.1084\n",
      "Epoch 4/20\n",
      "111/111 [==============================] - 0s 3ms/step - loss: 3.7103 - accuracy: 0.1167\n",
      "Epoch 5/20\n",
      "111/111 [==============================] - 0s 3ms/step - loss: 2.6988 - accuracy: 0.1183\n",
      "Epoch 6/20\n",
      "111/111 [==============================] - 0s 3ms/step - loss: 2.4887 - accuracy: 0.1199\n",
      "Epoch 7/20\n",
      "111/111 [==============================] - 0s 3ms/step - loss: 2.4201 - accuracy: 0.1210\n",
      "Epoch 8/20\n",
      "111/111 [==============================] - 0s 2ms/step - loss: 2.3945 - accuracy: 0.1215\n",
      "Epoch 9/20\n",
      "111/111 [==============================] - 0s 1ms/step - loss: 2.3846 - accuracy: 0.1220\n",
      "Epoch 10/20\n",
      "111/111 [==============================] - 0s 1ms/step - loss: 2.3791 - accuracy: 0.1224\n",
      "Epoch 11/20\n",
      "111/111 [==============================] - ETA: 0s - loss: 2.3746 - accuracy: 0.12 - 0s 1ms/step - loss: 2.3723 - accuracy: 0.1225\n",
      "Epoch 12/20\n",
      "111/111 [==============================] - 0s 1ms/step - loss: 2.3736 - accuracy: 0.1224\n",
      "Epoch 13/20\n",
      "111/111 [==============================] - 0s 1ms/step - loss: 2.3713 - accuracy: 0.1226\n",
      "Epoch 14/20\n",
      "111/111 [==============================] - 0s 1ms/step - loss: 2.3646 - accuracy: 0.1229\n",
      "Epoch 15/20\n",
      "111/111 [==============================] - 0s 1ms/step - loss: 2.3641 - accuracy: 0.1229\n",
      "Epoch 16/20\n",
      "111/111 [==============================] - 0s 2ms/step - loss: 2.3636 - accuracy: 0.1229\n",
      "Epoch 17/20\n",
      "111/111 [==============================] - 0s 1ms/step - loss: 2.3607 - accuracy: 0.1231\n",
      "Epoch 18/20\n",
      "111/111 [==============================] - 0s 1ms/step - loss: 2.3609 - accuracy: 0.1231\n",
      "Epoch 19/20\n",
      "111/111 [==============================] - 0s 1ms/step - loss: 2.3591 - accuracy: 0.1236\n",
      "Epoch 20/20\n",
      "111/111 [==============================] - 0s 1ms/step - loss: 2.3589 - accuracy: 0.1234\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e860cf5760>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann.compile(optimizer= 'adam',loss='categorical_crossentropy', metrics = ['accuracy'])\n",
    "ann.fit(X_train,Y_train, batch_size=256, epochs = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbfe472",
   "metadata": {},
   "source": [
    "# Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7fdb7950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 66)                33066     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 66)                4422      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 11)                737       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 38,225\n",
      "Trainable params: 38,225\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ann.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69a61ac",
   "metadata": {},
   "source": [
    "# Final Inference\n",
    "\n",
    "Since the accuracy is very low compared to other models, the ANN model needs to be tuned in right way to get better results\n",
    "\n",
    "# Relevant Hyperparameters to tune:\n",
    "\n",
    "a. Number of nodes and hidden layers\n",
    "\n",
    "b. Number of units in a Dense layer\n",
    "\n",
    "c. Weight Initialization\n",
    "\n",
    "d. Activation Functions\n",
    "\n",
    "e. Learning Rate\n",
    "\n",
    "f. Number of Epochs and Batch Size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac16f5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
